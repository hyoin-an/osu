{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /Users/hyoinan/OneDrive - The Ohio State University/[2019-2024 OSU]/3-2022-Spring/STAT7620/Project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "# import nltk\n",
    "# def ExtractNP(text):\n",
    "#     nounphrases = []\n",
    "#     words = nltk.word_tokenize(text)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "\n",
    "#     grammar = r\"\"\"\n",
    "#         NP:\n",
    "#             {<JJ*><NN+><IN><NN>}\n",
    "#             {<NN.*|JJ>*<NN.*>}\n",
    "#         \"\"\"\n",
    "#     chunkParser = nltk.RegexpParser(grammar)\n",
    "#     tree = chunkParser.parse(tagged)\n",
    "#     for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "#         myPhrase = ''\n",
    "#         for item in subtree.leaves():\n",
    "#             myPhrase += ' ' + item[0]\n",
    "#         nounphrases.append(myPhrase.strip())\n",
    "#         # print(myPhrase)\n",
    "#     nounphrases = list(filter(lambda x: len(x.split()) > 1, nounphrases))\n",
    "#     return nounphrases\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# test = TfidfVectorizer(dat1[0:10])\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "\n",
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict \n",
    "\n",
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "\n",
    "def transform(dataset,vocabulary,idf_values):\n",
    "     sparse_matrix= csr_matrix((len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "     for row  in range(0,len(dataset)):\n",
    "       number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "       for word in dataset[row].split():\n",
    "           if word in  list(vocabulary.keys()):\n",
    "               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "               sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
    "    #  print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
    "     output = normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "     return output\n",
    "\n",
    "\n",
    "import TeKET\n",
    "from TeKET import Tree\n",
    "from TeKET import Node\n",
    "# from TeKET import Root\n",
    "from TeKET import TreeManager\n",
    "from TeKET import ExtractCandidate\n",
    "from TeKET import NounTFCalculation\n",
    "\n",
    "# import TeKET_comments\n",
    "# from TeKET_comments import Tree\n",
    "# from TeKET_comments import Node\n",
    "# # from TeKET import Root\n",
    "# from TeKET_comments import TreeManager\n",
    "# from TeKET_comments import ExtractCandidate\n",
    "# from TeKET_comments import NounTFCalculation\n",
    "\n",
    "# import TFIDF_train_data\n",
    "# from TFIDF_train_data import TFIDF\n",
    "# from TFIDF_train_data import PrecisionRecallF1Calculation\n",
    "# from TFIDF_train_data import DataProcessing\n",
    "# import TFIDF_test_data\n",
    "\n",
    "\n",
    "def find_index(word, phrase):\n",
    "    i = 0\n",
    "    for w in phrase:\n",
    "        if w == word:\n",
    "            return(i)\n",
    "            break\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyoinan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "file1 = open(\"task05-TRAIN/train/H-37.txt\", \"r\")\n",
    "count = 0\n",
    "dat1 = np.array([])\n",
    " \n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    dat1 = np.append(dat1, line.strip())\n",
    "\n",
    "my_str = \"\"\n",
    "for line in dat1:\n",
    "    my_str = my_str + \" \" + str(line)\n",
    "# my_str = my_str.split(\". \")\n",
    "# dat1 = np.array(my_str)\n",
    "\n",
    "# candidate = np.array([])\n",
    "# for sen in dat1:\n",
    "#     candidate_phrase = ExtractNP(sen)\n",
    "#     candidate = np.append(candidate, candidate_phrase)\n",
    "\n",
    "candidate = ExtractCandidate(my_str)\n",
    "candidate_phrases = candidate.extract_candidate_chunks()\n",
    "# print(candidate.extract_candidate_chunks())\n",
    "candidate = candidate.CleaningCandidatePhrases(candidate_phrases) # To make the phrase of the basic form\n",
    "\n",
    "Vocabulary, idf_of_vocabulary = fit(candidate) \n",
    "\n",
    "final_output = transform(candidate, Vocabulary, idf_of_vocabulary) # TF-IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Similar phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation (root, final_output, candidate):\n",
    "\n",
    "    similar_candidate = []\n",
    "    sen_ind = []\n",
    "    sen_number = 0\n",
    "    for sen in candidate:\n",
    "        if sen.split().count(root)>0:\n",
    "            similar_candidate.append(sen)\n",
    "            sen_ind.append(sen_number)\n",
    "        sen_number = sen_number + 1\n",
    "    # the first element will be used to construct a tree, and the rest of it will be used to grow/prune the tree\n",
    "\n",
    "    # # Solution for (1) & (2): removing dulicate words & the non-alphabetic words from each sentence in similar candidate phrases\n",
    "    # similar_candidate_clean = []\n",
    "    # for sen in similar_candidate:\n",
    "    #     words = sen.split() # [( ), ( ), ..., ( )] \n",
    "    #     sen = \" \".join(sorted(set(words), key=words.index)) # removing dulicate words\n",
    "    #     sen_clean = []\n",
    "    #     for word in sen.split():\n",
    "    #         if (word.isalpha() or word.isspace()) == True:\n",
    "    #             sen_clean.append(word)\n",
    "    #     sen_clean = \" \".join(str(e) for e in sen_clean)\n",
    "    #     similar_candidate_clean.append(sen_clean)\n",
    "    # similar_candidate = similar_candidate_clean\n",
    "\n",
    "    # Solution for (1): removing dulicate words\n",
    "    similar_candidate_clean = []\n",
    "    for sen in similar_candidate:\n",
    "        words = sen.split() # [( ), ( ), ..., ( )] \n",
    "        sen = \" \".join(sorted(set(words), key=words.index)) # removing dulicate words\n",
    "        similar_candidate_clean.append(sen)\n",
    "    similar_candidate = similar_candidate_clean\n",
    "\n",
    "    # Find TF-IDF values for the similar candidate\n",
    "    index_matrix = np.array(final_output.nonzero())\n",
    "    similar_TFIDF = []\n",
    "    for row_ind in sen_ind:\n",
    "        ind = (index_matrix[0,] == row_ind) # find the corresponding phrase\n",
    "        col_ind = index_matrix[1, ind]\n",
    "        tf_idf_k = np.asarray(final_output[row_ind, col_ind].todense()).flatten()\n",
    "        similar_TFIDF.append(tf_idf_k)\n",
    "\n",
    "    # Solution for (2) & (3): Removing the problematic sentences for now\n",
    "    remove_list_ind = []\n",
    "    for l in range(len(similar_candidate)):\n",
    "        sim_sen = similar_candidate[l].split()\n",
    "        logic = (len(sim_sen) == len(similar_TFIDF[l]))\n",
    "        if logic == False:\n",
    "            # print('[phrase ' + str(l) + '] number of words: ' + str(len(similar_candidate[l].split())) + ', number of TFIDFs: ' + str(len(similar_TFIDF[l])))\n",
    "            remove_list_ind.append(l)\n",
    "    remove_list_ind.sort(reverse=True, key=int)\n",
    "    for i in remove_list_ind:\n",
    "        del similar_candidate[int(i)]\n",
    "        del similar_TFIDF[int(i)]\n",
    "\n",
    "    # print(similar_TFIDF)\n",
    "    # print(similar_candidate[0].split())\n",
    "    sigma1_index = similar_candidate[0].split().index(root)\n",
    "    root_tfidf = similar_TFIDF[0][sigma1_index]\n",
    "\n",
    "    return similar_candidate, similar_TFIDF, root_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['svm', 'spam', 'onlin', 'data', 'set', 'perform', 'result', 'exampl', 'detect', 'method', 'valu', 'email', 'blog', 'cost', 'rosvm', 'vector', 'number', 'margin', 'comput', 'featur', 'splog', 'time', 'high', 'section', 'machin', 'task', 'comment', 'size', 'test', 'optim', 'word', 'iter', 'figur', 'filter', 'train', 'smo', 'text', 'updat', 'hyperplan', 'experi', 'relax', 'problem', 'content-bas', 'effect', 'hypothesi', 'proceed', 'support', 'classif', 'paramet', 'benchmark', 'note', 'messag', 'confer', 'previou', 'trec', 'comparison', 'use', 'larg', 'content', 'linear', 'maximum', 'space', 'same', 'seendata', 'trec06p']\n"
     ]
    }
   ],
   "source": [
    "TF = NounTFCalculation()\n",
    "TF = TF.NounTF(candidate)\n",
    "# print(TF) # possible roots?\n",
    "\n",
    "sorted_TF = sorted(TF, key=lambda tf: tf[1], reverse=True)\n",
    "root_word_list = [x[0] for x in sorted_TF]\n",
    "print(root_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF = NounTFCalculation()\n",
    "\n",
    "# sorted_TF = sorted(TF, key=lambda tf: tf[1], reverse=True)\n",
    "# #print(sorted_TF)\n",
    "\n",
    "# root_list = [x[0] for x in sorted_TF]\n",
    "\n",
    "# # Remove the root words that contain non-alphabetic components \n",
    "# new_root_list = []\n",
    "# for root in root_list: \n",
    "#     if (root.isalpha() or root.isspace()) == True: \n",
    "#         new_root_list.append(root)\n",
    "# #print(len(new_root_list))\n",
    "\n",
    "# root_list = new_root_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMuValueFromTree(root, mu_dict):\n",
    "        if root:\n",
    "            GetMuValueFromTree(root.prev, mu_dict)\n",
    "            mu_dict[root.word] = root.mu\n",
    "            GetMuValueFromTree(root.next, mu_dict)\n",
    "\n",
    "def FindPhraseInCandidate(phrase, candidate_phrases):\n",
    "        if len(phrase) > 0:\n",
    "            for i in range (0, len(candidate_phrases)):\n",
    "                if phrase == candidate_phrases[i]:\n",
    "                    return i\n",
    "            return -1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration completed...\n",
      "2th iteration completed...\n",
      "3th iteration completed...\n",
      "4th iteration completed...\n",
      "5th iteration completed...\n",
      "6th iteration completed...\n",
      "7th iteration completed...\n",
      "8th iteration completed...\n",
      "9th iteration completed...\n",
      "10th iteration completed...\n",
      "11th iteration completed...\n",
      "12th iteration completed...\n",
      "13th iteration completed...\n",
      "14th iteration completed...\n",
      "15th iteration completed...\n",
      "16th iteration completed...\n",
      "17th iteration completed...\n",
      "18th iteration completed...\n",
      "19th iteration completed...\n",
      "20th iteration completed...\n",
      "21th iteration completed...\n",
      "22th iteration completed...\n",
      "23th iteration completed...\n",
      "24th iteration completed...\n",
      "25th iteration completed...\n",
      "26th iteration completed...\n",
      "27th iteration completed...\n",
      "28th iteration completed...\n",
      "29th iteration completed...\n",
      "30th iteration completed...\n",
      "31th iteration completed...\n",
      "32th iteration completed...\n",
      "33th iteration completed...\n",
      "34th iteration completed...\n",
      "35th iteration completed...\n",
      "36th iteration completed...\n",
      "37th iteration completed...\n",
      "38th iteration completed...\n",
      "39th iteration completed...\n",
      "40th iteration completed...\n",
      "41th iteration completed...\n",
      "42th iteration completed...\n",
      "43th iteration completed...\n",
      "44th iteration completed...\n",
      "45th iteration completed...\n",
      "46th iteration completed...\n",
      "47th iteration completed...\n",
      "48th iteration completed...\n",
      "49th iteration completed...\n",
      "50th iteration completed...\n",
      "51th iteration completed...\n",
      "52th iteration completed...\n",
      "53th iteration completed...\n",
      "54th iteration completed...\n",
      "55th iteration completed...\n",
      "56th iteration completed...\n",
      "57th iteration completed...\n",
      "58th iteration completed...\n",
      "59th iteration completed...\n",
      "60th iteration completed...\n",
      "61th iteration completed...\n",
      "62th iteration completed...\n",
      "63th iteration completed...\n",
      "64th iteration completed...\n",
      "65th iteration completed...\n"
     ]
    }
   ],
   "source": [
    "my_final_list = []\n",
    "weights = []\n",
    "my_final_dict = {}\n",
    "\n",
    "mu = -30\n",
    "\n",
    "for i in range(len(root_word_list)):\n",
    "    # print(\"i = \" + str(i))\n",
    "    root_word = root_word_list[i]\n",
    "    similar_candidate, similar_TFIDF, root_tfidf = preparation(root = root_word, final_output = final_output, candidate = candidate)\n",
    "\n",
    "    rootphrase = similar_candidate[0].split()\n",
    "    rootIndex = find_index(root_word, rootphrase)\n",
    "\n",
    "    my_tree = Tree()\n",
    "    my_tree.AddNode(word = root_word, phrase = rootphrase, wordIndex = rootIndex, rootIndex = rootIndex, tfidf = root_tfidf) # Add the root first\n",
    "\n",
    "    for j in range(len(similar_candidate)):\n",
    "        phrase = similar_candidate[j].split()\n",
    "        rootIndex = find_index(root_word, phrase)   # need to find rootIndex in each phrase\n",
    "        # print(\"j = \" + str(j))\n",
    "        for wordIndex in range(len(phrase)):\n",
    "            word = phrase[wordIndex]                    # extract the first word from the phrase with wordIndex\n",
    "            word_tfidf = similar_TFIDF[j][wordIndex]    # selecting the first sentence\n",
    "            \n",
    "            my_tree.AddNode(word, phrase, wordIndex, rootIndex, word_tfidf) \n",
    "            # my_tree.PrintTree(my_tree.root)\n",
    "            # print(str(wordIndex) + \"th iteration completed...\")\n",
    "\n",
    "        my_tree.UpdateMuValues(phrase)\n",
    "        \n",
    "    # my_tree.PrintTree(my_tree.root)\n",
    "    # my_NodeList = my_tree.CreateNodeList(my_tree.root)\n",
    "    # print(my_NodeList)\n",
    "\n",
    "    # Getting the final phrases list\n",
    "    final_node_list = []\n",
    "    final_node_list = my_tree.FindNodeListToExtractKeyPhrases(mu = mu, final_node_list = final_node_list, candidate_phrases= candidate) # This includes tree pruning too\n",
    "    final_phrases = []\n",
    "    for j in range(0, len(final_node_list)):\n",
    "        final_phrases.append(my_tree.GetPhrase(final_node_list[j]))\n",
    "    # my_final_list.append(final_phrases)\n",
    "\n",
    "\n",
    "    ## Getting the final mu list\n",
    "\n",
    "    # Get the mu value for each word\n",
    "    mu_dict = {} # a dictionary that saves \"word : mu\" pairs \n",
    "    GetMuValueFromTree(my_tree.root, mu_dict)\n",
    "\n",
    "    # Save the final keyphrases and their weights\n",
    "    for l in range(0, len(final_phrases)):\n",
    "        k = FindPhraseInCandidate(final_phrases[l], similar_candidate)\n",
    "        if  k == -1: \n",
    "            continue\n",
    "        else:\n",
    "            my_final_list.append(final_phrases[l])\n",
    "\n",
    "            # calculate the sum of tfidf of all the words in the final keyphrase\n",
    "            tfidf_value = np.sum(similar_TFIDF[k])\n",
    "            # calculate the sum of mu of all the words in the final keyphrase\n",
    "            mu_value = 0 \n",
    "            for j in final_phrases[l].split():\n",
    "                mu_value = mu_value + mu_dict[j]\n",
    "            weights.append(tfidf_value * mu_value)\n",
    "    \n",
    "    for m in range(0, len(my_final_list)): \n",
    "        my_final_dict[my_final_list[m]] = weights[m]\n",
    "    print(str(i+1) + \"th iteration completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm': 173.0,\n",
       " 'spam': 170.0,\n",
       " 'onlin': 110.0,\n",
       " 'data mine': 107.03513161388946,\n",
       " 'data': 97.0,\n",
       " 'set': 74.0,\n",
       " 'comput cost': 73.51550573878438,\n",
       " 'perform': 73.0,\n",
       " 'spam comment': 70.72420572990819,\n",
       " 'rosvm test': 70.57540843056644,\n",
       " 'support vector machin': 69.0824633143468,\n",
       " 'result': 69.0,\n",
       " 'detect': 59.0,\n",
       " 'exampl xi': 58.65741414907701,\n",
       " 'method trade': 57.844300807508,\n",
       " 'margin maxim': 54.02335043906604,\n",
       " 'exampl': 54.0,\n",
       " 'vector machin': 53.685064694791016,\n",
       " 'method': 52.0,\n",
       " 'valu': 51.0,\n",
       " 'email': 51.0,\n",
       " 'benchmark data set': 49.531327081792206,\n",
       " 'blog': 49.0,\n",
       " 'cost': 49.0,\n",
       " 'rosvm': 48.0,\n",
       " 'splog detect': 43.779988085186915,\n",
       " 'vector': 42.0,\n",
       " 'contentbas detect': 39.82670598659872,\n",
       " 'word vector': 39.53729646210428,\n",
       " 'number': 39.0,\n",
       " 'margin': 39.0,\n",
       " 'spam filter': 38.639081951240364,\n",
       " 'comput': 38.0,\n",
       " 'featur': 36.0,\n",
       " 'method trade statist robust': 35.445632473717154,\n",
       " 'splog': 35.0,\n",
       " 'support vector learn': 34.44944253419359,\n",
       " 'blog identif': 34.283856990890094,\n",
       " 'time': 34.0,\n",
       " 'text classif': 33.93776693461865,\n",
       " 'current support vector': 32.360885623255925,\n",
       " 'maximum number': 32.340812781480196,\n",
       " 'size test': 31.112509343493002,\n",
       " 'section': 29.0,\n",
       " 'machin': 28.0,\n",
       " 'task': 28.0,\n",
       " 'optim stabil': 27.78207202012313,\n",
       " 'comment spam detect experi': 27.599865616044674,\n",
       " 'machin learn': 26.77696475785677,\n",
       " 'uniqu spam filter': 26.68671680205776,\n",
       " 'expens comput': 26.5884750859208,\n",
       " 'onlin smo': 26.476942689226387,\n",
       " 'iter solver': 25.197918164091014,\n",
       " 'size': 25.0,\n",
       " 'test': 25.0,\n",
       " 'optim': 25.0,\n",
       " 'word': 25.0,\n",
       " 'smo iter': 24.038674414845136,\n",
       " 'iter': 24.0,\n",
       " 'test set': 23.894802731517327,\n",
       " 'onlin spam filter': 23.77007087010638,\n",
       " 'updat as': 23.70869809553873,\n",
       " 'section demonstr': 23.557276189194763,\n",
       " 'addit iter': 22.398149479192014,\n",
       " 'hyperplan closer': 22.314068795801155,\n",
       " 'high level': 22.2743200134749,\n",
       " 'figur': 21.0,\n",
       " 'filter': 21.0,\n",
       " 'content analysi': 19.79391096106386,\n",
       " 'smo': 19.0,\n",
       " 'text': 19.0,\n",
       " 'updat': 19.0,\n",
       " 'experi': 19.0,\n",
       " 'relax': 18.0,\n",
       " 'problem': 18.0,\n",
       " 'effect': 18.0,\n",
       " 'proceed': 18.0,\n",
       " 'hyperplan': 17.0,\n",
       " 'comput save': 16.545680691332024,\n",
       " 'support': 16.0,\n",
       " 'trec05p-1 figur': 15.492863473706851,\n",
       " 'paramet valu': 15.467227471009602,\n",
       " 'lem size': 15.270614540937519,\n",
       " 'origin onlin smo': 15.156636142998117,\n",
       " 'hypothesi': 15.0,\n",
       " 'maximum margin': 14.061222948469648,\n",
       " 'experiment section': 14.039837226628393,\n",
       " 'note': 14.0,\n",
       " 'content-bas splog detect': 13.7989485582827,\n",
       " 'maximum margin requir': 13.75480838501164,\n",
       " 'messag': 13.0,\n",
       " 'bayesian classif': 12.711079027347154,\n",
       " 'trec': 12.0,\n",
       " 'problem size': 11.308084362216448,\n",
       " 'large-scal content': 11.289226804655375,\n",
       " 'linear time': 11.263640526359154,\n",
       " 'binari featur': 11.203174364950975,\n",
       " 'refer experi': 11.200107667560879,\n",
       " 'comparison result': 11.172799726786847,\n",
       " 'use': 11.0,\n",
       " 'content': 11.0,\n",
       " 'seendata': 11.0,\n",
       " 'paramet': 10.0,\n",
       " 'previou optim': 9.88173936917446,\n",
       " 'train data': 9.7824573445702,\n",
       " 'execut time': 9.672419411236085,\n",
       " 'trec06p': 9.0,\n",
       " 'trec05p-1 trec06p onsvm': 8.643281812100707,\n",
       " 'paper use': 8.484392213904071,\n",
       " 'initi hypothesi': 8.439692494665561,\n",
       " 're-train updat': 8.400080750670659,\n",
       " 'trec05p-1 trec06p': 8.101993187042758,\n",
       " 'default paramet': 7.0396462933774355,\n",
       " 'classif perform': 6.999278413417429,\n",
       " 'maximum margin hyperplan': 6.901785798228346,\n",
       " 'third confer': 5.651337546426207,\n",
       " 'previou score': 5.642105304048561,\n",
       " 'seendata add xi': 5.173020099003388,\n",
       " 'apples-to-appl comparison': 4.205859299618685,\n",
       " 'long messag': 4.202516617033437,\n",
       " 'decis hyperplan': 4.183887899212717,\n",
       " 'same data': 4.156318123786619,\n",
       " 'relax requir': 1.409715865582078,\n",
       " 'trec winner': 0.0,\n",
       " 'key problem': -1.3956119166131558,\n",
       " 'content-bas blog': -7.041370376706137,\n",
       " 'larg blog host': -39.00128633285416}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_final_dict_sorted = dict(sorted(my_final_dict.items(), key=lambda item: item[1], reverse = True))\n",
    "my_final_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = 0\n",
    "# cnt = 0\n",
    "# for c in range(len(candidate)):\n",
    "#     sent = candidate[c]\n",
    "#     if sent == 'resourc':\n",
    "#         cnt = cnt + 1\n",
    "#         # print(ind)\n",
    "#     ind = ind + 1\n",
    "# print(cnt)\n",
    "\n",
    "# candidate[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = list(my_final_dict_sorted.keys())\n",
    "# value = list(my_final_dict_sorted.values())\n",
    "\n",
    "# final_real_phrases = {}\n",
    "# for i in range(len(key)):\n",
    "#     if len(key[i].split()) > 1:\n",
    "#         final_real_phrases[key[i]] = value[i]\n",
    "# final_real_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Using ExtractCandidate()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm',\n",
       " 'spam',\n",
       " 'onlin',\n",
       " 'data mine',\n",
       " 'data',\n",
       " 'set',\n",
       " 'comput cost',\n",
       " 'perform',\n",
       " 'spam comment',\n",
       " 'rosvm test',\n",
       " 'support vector machin',\n",
       " 'result',\n",
       " 'detect',\n",
       " 'exampl xi',\n",
       " 'method trade',\n",
       " 'margin maxim',\n",
       " 'exampl',\n",
       " 'vector machin',\n",
       " 'method',\n",
       " 'valu']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_real_phrases = my_final_dict_sorted\n",
    "\n",
    "N = 20\n",
    "N_phrases = list(final_real_phrases.keys())[0:N]\n",
    "print(\"(1) Using ExtractCandidate()\")\n",
    "N_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard = ['support vector machin', 'content-base filter', 'spam filter', 'blog', 'splog', 'link analysi', \n",
    "'machin learn techniqu', 'link spam', 'content-base spam detect', 'bayesian method', 'increment updat', \n",
    "'logist regress', 'hyperplan', 'featur map', 'spam filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_correct = 0\n",
    "for word in N_phrases:\n",
    "    if word in standard:\n",
    "        K_correct = K_correct + 1\n",
    "K_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Standard (Stem version)\n",
    "\n",
    "[author]\n",
    "distribut real-time emb system,hybrid system,qualiti of servic+servic qualiti\n",
    "\n",
    "[reader]\n",
    "adapt resourc manag,distribut real-time embed system,end-to-end qualiti of servic+servic end-to-end qualiti,hybrid adapt resourcemanag middlewar,hybrid control techniqu,real-time video distribut system,real-time corba specif,video encod/decod,resourc reserv mechan,dynam environ,stream servic\n",
    "\n",
    "\n",
    "[Combined]\n",
    "adapt resourc manag,distribut real-time embed system,end-to-end qualiti of servic+servic end-to-end qualiti,hybrid adapt resourcemanag middlewar,hybrid control techniqu,real-time video distribut system,real-time corba specif,video encod/decod,resourc reserv mechan,dynam environ,stream servic,distribut real-time emb system,hybrid system,qualiti of servic+servic qualiti"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "025ad61b930c2538e978b72ab3450d0e2eb24a8c556b2730cae258c3bfbd449a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
