{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /Users/hyoinan/OneDrive - The Ohio State University/[2019-2024 OSU]/3-2022-Spring/STAT7620/Project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "import nltk\n",
    "def ExtractNP(text):\n",
    "    nounphrases = []\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "        NP:\n",
    "            {<JJ*><NN+><IN><NN>}\n",
    "            {<NN.*|JJ>*<NN.*>}\n",
    "        \"\"\"\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    tree = chunkParser.parse(tagged)\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        myPhrase = ''\n",
    "        for item in subtree.leaves():\n",
    "            myPhrase += ' ' + item[0]\n",
    "        nounphrases.append(myPhrase.strip())\n",
    "        # print(myPhrase)\n",
    "    nounphrases = list(filter(lambda x: len(x.split()) > 1, nounphrases))\n",
    "    return nounphrases\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# test = TfidfVectorizer(dat1[0:10])\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "\n",
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict \n",
    "\n",
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "\n",
    "def transform(dataset,vocabulary,idf_values):\n",
    "     sparse_matrix= csr_matrix((len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "     for row  in range(0,len(dataset)):\n",
    "       number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "       for word in dataset[row].split():\n",
    "           if word in  list(vocabulary.keys()):\n",
    "               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "               sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
    "    #  print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
    "     output = normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "     return output\n",
    "\n",
    "\n",
    "import TeKET\n",
    "from TeKET import Tree\n",
    "from TeKET import Node\n",
    "# from TeKET import Root\n",
    "from TeKET import TreeManager\n",
    "from TeKET import ExtractCandidate\n",
    "from TeKET import NounTFCalculation\n",
    "\n",
    "# import TeKET_comments\n",
    "# from TeKET_comments import Tree\n",
    "# from TeKET_comments import Node\n",
    "# # from TeKET import Root\n",
    "# from TeKET_comments import TreeManager\n",
    "# from TeKET_comments import ExtractCandidate\n",
    "# from TeKET_comments import NounTFCalculation\n",
    "\n",
    "# import TFIDF_train_data\n",
    "# from TFIDF_train_data import TFIDF\n",
    "# from TFIDF_train_data import PrecisionRecallF1Calculation\n",
    "# from TFIDF_train_data import DataProcessing\n",
    "# import TFIDF_test_data\n",
    "\n",
    "\n",
    "def find_index(word, phrase):\n",
    "    i = 0\n",
    "    for w in phrase:\n",
    "        if w == word:\n",
    "            return(i)\n",
    "            break\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyoinan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/scipy/sparse/_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "file1 = open(\"task05-TRAIN/train/H-37.txt\", \"r\")\n",
    "count = 0\n",
    "dat1 = np.array([])\n",
    " \n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    dat1 = np.append(dat1, line.strip())\n",
    "\n",
    "my_str = \"\"\n",
    "for line in dat1:\n",
    "    my_str = my_str + \" \" + str(line)\n",
    "my_str = my_str.split(\". \")\n",
    "dat1 = np.array(my_str)\n",
    "\n",
    "# candidate = np.array([])\n",
    "# for sen in dat1:\n",
    "#     candidate_phrase = ExtractNP(sen)\n",
    "#     candidate = np.append(candidate, candidate_phrase)\n",
    "\n",
    "# candidate = candidate.tolist()\n",
    "\n",
    "# candidate = ExtractCandidate(my_str)\n",
    "# candidate_phrases = candidate.extract_candidate_chunks()\n",
    "# # print(candidate.extract_candidate_chunks())\n",
    "# candidate = CleaningCandidatePhrases(candidate_phrases) # To make the phrase of the basic form\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def CleaningCandidatePhrases(probable_phrases):\n",
    "        probpr = []\n",
    "        \n",
    "        for i in range (0, len(probable_phrases), 1):\n",
    "            words = nltk.word_tokenize(probable_phrases[i]) \n",
    "            wa = []\n",
    "            for w in words:\n",
    "                x = ps.stem(w)\n",
    "                wa.append(x)\n",
    "            probable_phrases[i] = (' '. join(wa))\n",
    "            \n",
    "            #probpr.append(probable_phrases[i])\n",
    "        \n",
    "        for i in range (0, len(probable_phrases), 1):\n",
    "            for j in range(0,len(probable_phrases[i]),1):\n",
    "                regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:].')\n",
    "                if regex.search(probable_phrases[i]) == None:\n",
    "                    probpr.append(probable_phrases[i])\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "        for i in range (0, len(probable_phrases), 1):\n",
    "            temp_probable_phrases = probable_phrases[i].split(\" \")\n",
    "            #print(temp_probable_phrases)\n",
    "            min_length = True\n",
    "            for k in range(0,len(temp_probable_phrases),1):\n",
    "                if len(temp_probable_phrases[k]) < 2:\n",
    "                    min_length = False\n",
    "            \n",
    "            \n",
    "            if min_length:\n",
    "                count = 0\n",
    "                for j in range (0, len(probable_phrases), 1):\n",
    "                    if probable_phrases[i] == probable_phrases[j]:\n",
    "                        count = count + 1\n",
    "            \n",
    "                    if count > 2:\n",
    "                        probpr.append(probable_phrases[i]) \n",
    "                        #print(probable_phrases[i])\n",
    "                        break  \n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "        return probpr    \n",
    "\n",
    "candidate = np.array([])\n",
    "for sen in dat1:\n",
    "    candidate_phrases = CleaningCandidatePhrases(ExtractNP(sen))\n",
    "    candidate = np.append(candidate, candidate_phrases)\n",
    "\n",
    "candidate = candidate.tolist()\n",
    "\n",
    "Vocabulary, idf_of_vocabulary = fit(candidate) \n",
    "\n",
    "final_output = transform(candidate, Vocabulary, idf_of_vocabulary) # TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation (root, final_output, candidate):\n",
    "\n",
    "    similar_candidate = []\n",
    "    sen_ind = []\n",
    "    sen_number = 0\n",
    "    for sen in candidate:\n",
    "        if sen.split().count(root)>0:\n",
    "            similar_candidate.append(sen)\n",
    "            sen_ind.append(sen_number)\n",
    "        sen_number = sen_number + 1\n",
    "    # # the first element will be used to construct a tree, and the rest of it will be used to grow/prune the tree\n",
    "\n",
    "    # Solution for (1): removing dulicate words\n",
    "    similar_candidate_clean = []\n",
    "    for sen in similar_candidate:\n",
    "        words = sen.split()\n",
    "        sen = \" \".join(sorted(set(words), key=words.index))\n",
    "        similar_candidate_clean.append(sen)\n",
    "    similar_candidate = similar_candidate_clean\n",
    "\n",
    "    # Find TF-IDF values for the similar candidate\n",
    "    index_matrix = np.array(final_output.nonzero())\n",
    "    similar_TFIDF = []\n",
    "    for row_ind in sen_ind:\n",
    "        ind = (index_matrix[0,] == row_ind) # find the corresponding phrase\n",
    "        col_ind = index_matrix[1, ind]\n",
    "        tf_idf_k = np.asarray(final_output[row_ind, col_ind].todense()).flatten()\n",
    "        similar_TFIDF.append(tf_idf_k)\n",
    "\n",
    "    # Solution for (2) & (3): Removing the problematic sentences for now\n",
    "    remove_list_ind = []\n",
    "    for l in range(len(similar_candidate)):\n",
    "        logic = (len(similar_candidate[l].split()) == len(similar_TFIDF[l]))\n",
    "        if logic == False:\n",
    "            # print('[phrase ' + str(l) + '] number of words: ' + str(len(similar_candidate[l].split())) + ', number of TFIDFs: ' + str(len(similar_TFIDF[l])))\n",
    "            remove_list_ind.append(l)\n",
    "    remove_list_ind.sort(reverse=True, key=int)\n",
    "    for i in remove_list_ind:\n",
    "        del similar_candidate[int(i)]\n",
    "        del similar_TFIDF[int(i)]\n",
    "\n",
    "    # print(similar_TFIDF)\n",
    "    sigma1_index = similar_candidate[0].split().index(root)\n",
    "    root_tfidf = similar_TFIDF[0][sigma1_index]\n",
    "\n",
    "    return similar_candidate, similar_TFIDF, root_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF = NounTFCalculation()\n",
    "# TF = TF.NounTF(candidate_phrases)\n",
    "# print(TF) # possible roots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam', 'svm', 'onlin', 'data', 'set', 'detect', 'perform', 'email', 'blog', 'vector', 'exampl', 'comput', 'content-bas', 'filter', 'method', 'train', 'valu', 'featur', 'high', 'cost', 'comment', 'machin', 'paramet', 'optim', 'splog', 'test', 'same', 'problem', 'text', 'relax', 'benchmark', 'time', 'learn']\n"
     ]
    }
   ],
   "source": [
    "TF = NounTFCalculation()\n",
    "TF = TF.NounTF(candidate)\n",
    "# print(TF) # possible roots?\n",
    "\n",
    "sorted_TF = sorted(TF, key=lambda tf: tf[1], reverse=True)\n",
    "root_word_list = [x[0] for x in sorted_TF]\n",
    "print(root_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMuValueFromTree(root, mu_dict):\n",
    "        if root:\n",
    "            GetMuValueFromTree(root.prev, mu_dict)\n",
    "            mu_dict[root.word] = root.mu\n",
    "            GetMuValueFromTree(root.next, mu_dict)\n",
    "\n",
    "def FindPhraseInCandidate(phrase, candidate_phrases):\n",
    "        if len(phrase) > 0:\n",
    "            for i in range (0, len(candidate_phrases)):\n",
    "                if phrase == candidate_phrases[i]:\n",
    "                    return i\n",
    "            return -1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration completed...\n",
      "2th iteration completed...\n",
      "3th iteration completed...\n",
      "4th iteration completed...\n",
      "5th iteration completed...\n",
      "6th iteration completed...\n",
      "7th iteration completed...\n",
      "8th iteration completed...\n",
      "9th iteration completed...\n",
      "10th iteration completed...\n",
      "11th iteration completed...\n",
      "12th iteration completed...\n",
      "13th iteration completed...\n",
      "14th iteration completed...\n",
      "15th iteration completed...\n",
      "16th iteration completed...\n",
      "17th iteration completed...\n",
      "18th iteration completed...\n",
      "19th iteration completed...\n",
      "20th iteration completed...\n",
      "21th iteration completed...\n",
      "22th iteration completed...\n",
      "23th iteration completed...\n",
      "24th iteration completed...\n",
      "25th iteration completed...\n",
      "26th iteration completed...\n",
      "27th iteration completed...\n",
      "28th iteration completed...\n",
      "29th iteration completed...\n",
      "30th iteration completed...\n",
      "31th iteration completed...\n",
      "32th iteration completed...\n",
      "33th iteration completed...\n"
     ]
    }
   ],
   "source": [
    "my_final_list = []\n",
    "weights = []\n",
    "my_final_dict = {}\n",
    "\n",
    "mu = -30\n",
    "\n",
    "for i in range(len(root_word_list)):\n",
    "    # print(\"i = \" + str(i))\n",
    "    root_word = root_word_list[i]\n",
    "    similar_candidate, similar_TFIDF, root_tfidf = preparation(root = root_word, final_output = final_output, candidate = candidate)\n",
    "\n",
    "    rootphrase = similar_candidate[0].split()\n",
    "    rootIndex = find_index(root_word, rootphrase)\n",
    "\n",
    "    my_tree = Tree()\n",
    "    my_tree.AddNode(word = root_word, phrase = rootphrase, wordIndex = rootIndex, rootIndex = rootIndex, tfidf = root_tfidf) # Add the root first\n",
    "\n",
    "    for j in range(len(similar_candidate)):\n",
    "        phrase = similar_candidate[j].split()\n",
    "        rootIndex = find_index(root_word, phrase)   # need to find rootIndex in each phrase\n",
    "        # print(\"j = \" + str(j))\n",
    "        for wordIndex in range(len(phrase)):\n",
    "            word = phrase[wordIndex]                    # extract the first word from the phrase with wordIndex\n",
    "            word_tfidf = similar_TFIDF[j][wordIndex]    # selecting the first sentence\n",
    "            \n",
    "            my_tree.AddNode(word, phrase, wordIndex, rootIndex, word_tfidf) \n",
    "            # my_tree.PrintTree(my_tree.root)\n",
    "            # print(str(wordIndex) + \"th iteration completed...\")\n",
    "\n",
    "        my_tree.UpdateMuValues(phrase)\n",
    "        \n",
    "    # my_tree.PrintTree(my_tree.root)\n",
    "    # my_NodeList = my_tree.CreateNodeList(my_tree.root)\n",
    "    # print(my_NodeList)\n",
    "\n",
    "    # Getting the final phrases list\n",
    "    final_node_list = []\n",
    "    final_node_list = my_tree.FindNodeListToExtractKeyPhrases(mu = mu, final_node_list = final_node_list, candidate_phrases= candidate) # This includes tree pruning too\n",
    "    final_phrases = []\n",
    "    for j in range(0, len(final_node_list)):\n",
    "        final_phrases.append(my_tree.GetPhrase(final_node_list[j]))\n",
    "    # my_final_list.append(final_phrases)\n",
    "\n",
    "\n",
    "    ## Getting the final mu list\n",
    "\n",
    "    # Get the mu value for each word\n",
    "    mu_dict = {} # a dictionary that saves \"word : mu\" pairs \n",
    "    GetMuValueFromTree(my_tree.root, mu_dict)\n",
    "\n",
    "    # Save the final keyphrases and their weights\n",
    "    for l in range(0, len(final_phrases)):\n",
    "        k = FindPhraseInCandidate(final_phrases[l], similar_candidate)\n",
    "        if  k == -1: \n",
    "            continue\n",
    "        else:\n",
    "            my_final_list.append(final_phrases[l])\n",
    "\n",
    "            # calculate the sum of tfidf of all the words in the final keyphrase\n",
    "            tfidf_value = np.sum(similar_TFIDF[k])\n",
    "            # calculate the sum of mu of all the words in the final keyphrase\n",
    "            mu_value = 0 \n",
    "            for j in final_phrases[l].split():\n",
    "                mu_value = mu_value + mu_dict[j]\n",
    "            weights.append(tfidf_value * mu_value)\n",
    "    \n",
    "    for m in range(0, len(my_final_list)): \n",
    "        my_final_dict[my_final_list[m]] = weights[m]\n",
    "    print(str(i+1) + \"th iteration completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contentbas spam': 98.63966598337315,\n",
       " 'content-bas spam detect': 89.02817394173105,\n",
       " 'large-scal content-bas spam detect': 76.08708019031403,\n",
       " 'spam detect': 70.14648337825203,\n",
       " 'data mine': 68.25308995798811,\n",
       " 'svm': 65.0,\n",
       " 'spam comment': 42.958887237994766,\n",
       " 'blog comment spam detect': 39.57373025243774,\n",
       " 'compar perform': 37.412963192279804,\n",
       " 'activ set': 36.592029755266175,\n",
       " 'spam filter': 36.10039606681223,\n",
       " 'blog identif': 34.500322092369515,\n",
       " 'benchmark data set': 29.085154391608146,\n",
       " 'comment spam detect experi': 27.14284343758393,\n",
       " 'expens comput': 26.66738746367917,\n",
       " 'uniqu spam filter': 23.381332591668198,\n",
       " 'high level': 22.224111389664692,\n",
       " 'onlin spam filter': 20.49895621650174,\n",
       " 'optim stabil': 19.525668558330533,\n",
       " 'method trade': 19.50971486029461,\n",
       " 'suport vector machin': 18.6785376848127,\n",
       " 'vector machin': 18.364134580450788,\n",
       " 'machin learn': 16.963742278093555,\n",
       " 'classifi exampl': 16.75538770736061,\n",
       " 'costli method': 15.249556668721373,\n",
       " 'comput save': 15.235192819779098,\n",
       " 'text classif': 14.140324993305718,\n",
       " 'email data': 14.106544529568733,\n",
       " 'n-gram vector': 13.981087158034828,\n",
       " 'paramet valu': 11.308733667351753,\n",
       " 'low valu': 11.101222537570994,\n",
       " 'support vector machin': 10.362583445523907,\n",
       " 'exampl xi': 9.846721142459753,\n",
       " 'problem size': 8.48265072500829,\n",
       " 'relax requir': 8.459961216006965,\n",
       " 'previou optim': 7.065690488686837,\n",
       " 'default paramet': 7.036587810548996,\n",
       " 'train data': 7.034052348586098,\n",
       " 'train cost': 5.656534490529999,\n",
       " 'test set': 5.59923348028817,\n",
       " 'execut time': 2.7985210771771376,\n",
       " 'same data': 1.3998083700720425,\n",
       " 'key problem': -1.3977064642864216,\n",
       " 'email data set': -1.7279847594423172,\n",
       " 'splog vs': -2.7923666305766726,\n",
       " 'method trade statist robust': -3.9645830278256176,\n",
       " 'svm learn': -4.159076510704926,\n",
       " 'empir test': -5.584733261153345,\n",
       " 'decrement support vector machin learn': -6.649451401122368,\n",
       " 'content-bas blog': -8.483836867956287,\n",
       " 'decrement svm learn': -23.514679519998932}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_final_dict_sorted = dict(sorted(my_final_dict.items(), key=lambda item: item[1], reverse = True))\n",
    "my_final_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = list(my_final_dict_sorted.keys())\n",
    "# value = list(my_final_dict_sorted.values())\n",
    "\n",
    "# final_real_phrases = {}\n",
    "# for i in range(len(key)):\n",
    "#     if len(key[i].split()) > 1:\n",
    "#         final_real_phrases[key[i]] = value[i]\n",
    "# final_real_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Using ExtractNP()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['contentbas spam',\n",
       " 'content-bas spam detect',\n",
       " 'large-scal content-bas spam detect',\n",
       " 'spam detect',\n",
       " 'data mine',\n",
       " 'svm',\n",
       " 'spam comment',\n",
       " 'blog comment spam detect',\n",
       " 'compar perform',\n",
       " 'activ set',\n",
       " 'spam filter',\n",
       " 'blog identif',\n",
       " 'benchmark data set',\n",
       " 'comment spam detect experi',\n",
       " 'expens comput']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_real_phrases = my_final_dict_sorted\n",
    "\n",
    "N = 15\n",
    "N_phrases = list(final_real_phrases.keys())[0:N]\n",
    "print(\"(2) Using ExtractNP()\")\n",
    "N_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['support vector machin',\n",
       " 'content-base filter',\n",
       " 'spam filter',\n",
       " 'blog',\n",
       " 'splog',\n",
       " 'link analysi',\n",
       " 'machin learn techniqu',\n",
       " 'link spam',\n",
       " 'content-base spam detect',\n",
       " 'bayesian method',\n",
       " 'increment updat',\n",
       " 'logist regress',\n",
       " 'hyperplan',\n",
       " 'featur map',\n",
       " 'spam filter']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H-37: K_correct = 1\n",
    "standard = ['support vector machin', 'content-base filter', 'spam filter', 'blog', 'splog', 'link analysi', \n",
    "'machin learn techniqu', 'link spam', 'content-base spam detect', 'bayesian method', 'increment updat', \n",
    "'logist regress', 'hyperplan', 'featur map', 'spam filter']\n",
    "N_standard = len(standard)\n",
    "standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # H-44: K_correct = 2\n",
    "# standard = \"time machin,text search,invert file index,tempor search,approxim tempor coalesc,web archiv,version document collect,collabor author environ,timestamp inform feed,document-content overlap,index rang-base valu,open sourc search-engin nutch,static indexprun techniqu,valid time-interv,sublist materi,tempor text index,time-travel text search\"\n",
    "# standard = standard.split(',')\n",
    "# N_standard = len(standard)\n",
    "# standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # H-49: K_correct = 3 (N = 15)\n",
    "# standard = \"perform predict,inform retriev,spatial autocorrel,autocorrel,cluster hypothesi,zero relev judgment,relationship of predictor+predictor relationship,predict power of predictor+predictor predict power,languag model score,rank of queri+queri rank,regular\"\n",
    "# standard = standard.split(',')\n",
    "# N_standard = len(standard)\n",
    "# standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # H-98: K_correct = 2\n",
    "# standard = \"bayesian risk model,empir score distribut,probabl estim,text classifi,parametr model,inform retriev,logist regress framework,posterior function,asymmetr laplac distribut,search engin retriev,symmetr distribut,asymmetr gaussian,maximum likelihood estim,class-condit densiti,decis threshold,text classif,cost-sensit learn,activ learn,classifi combin\"\n",
    "# standard = standard.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # J=61: K_correct = 1\n",
    "# standard = \"iter combinatori exchang,doubl auction,combinatori auction,buyer and seller,trade,tree-base bid languag,price,winner-determin,bid,threshold payment,combinatori exchang,vcg,prefer elicit\"\n",
    "# standard = standard.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06666666666666667\n",
      "Recall: 0.06666666666666667\n",
      "F1 Score: 0.06666666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spam filter']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_correct = 0\n",
    "correct_words = []\n",
    "for word in N_phrases:\n",
    "    if word in standard:\n",
    "        K_correct = K_correct + 1\n",
    "        correct_words.append(word)\n",
    "print('Precision: ' + str(K_correct/N))\n",
    "print('Recall: ' + str(K_correct/N_standard))\n",
    "print('F1 Score: ' + str((2 * K_correct/N * K_correct/N_standard)/(K_correct/N + K_correct/N_standard)))\n",
    "correct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "025ad61b930c2538e978b72ab3450d0e2eb24a8c556b2730cae258c3bfbd449a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
