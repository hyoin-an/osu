{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /Users/hyoinan/OneDrive - The Ohio State University/[2019-2024 OSU]/3-2022-Spring/STAT7620/Project\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "print(\"Current Working Directory \" , os.getcwd())\n",
    "\n",
    "# import nltk\n",
    "# def ExtractNP(text):\n",
    "#     nounphrases = []\n",
    "#     words = nltk.word_tokenize(text)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "\n",
    "#     grammar = r\"\"\"\n",
    "#         NP:\n",
    "#             {<JJ*><NN+><IN><NN>}\n",
    "#             {<NN.*|JJ>*<NN.*>}\n",
    "#         \"\"\"\n",
    "#     chunkParser = nltk.RegexpParser(grammar)\n",
    "#     tree = chunkParser.parse(tagged)\n",
    "#     for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "#         myPhrase = ''\n",
    "#         for item in subtree.leaves():\n",
    "#             myPhrase += ' ' + item[0]\n",
    "#         nounphrases.append(myPhrase.strip())\n",
    "#         # print(myPhrase)\n",
    "#     nounphrases = list(filter(lambda x: len(x.split()) > 1, nounphrases))\n",
    "#     return nounphrases\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# test = TfidfVectorizer(dat1[0:10])\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "\n",
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict \n",
    "\n",
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "\n",
    "def transform(dataset,vocabulary,idf_values):\n",
    "     sparse_matrix= csr_matrix((len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "     for row  in range(0,len(dataset)):\n",
    "       number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "       for word in dataset[row].split():\n",
    "           if word in  list(vocabulary.keys()):\n",
    "               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "               sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
    "    #  print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
    "     output = normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "     return output\n",
    "\n",
    "\n",
    "import TeKET\n",
    "from TeKET import Tree\n",
    "from TeKET import Node\n",
    "# from TeKET import Root\n",
    "from TeKET import TreeManager\n",
    "from TeKET import ExtractCandidate\n",
    "from TeKET import NounTFCalculation\n",
    "\n",
    "# import TeKET_comments\n",
    "# from TeKET_comments import Tree\n",
    "# from TeKET_comments import Node\n",
    "# # from TeKET import Root\n",
    "# from TeKET_comments import TreeManager\n",
    "# from TeKET_comments import ExtractCandidate\n",
    "# from TeKET_comments import NounTFCalculation\n",
    "\n",
    "# import TFIDF_train_data\n",
    "# from TFIDF_train_data import TFIDF\n",
    "# from TFIDF_train_data import PrecisionRecallF1Calculation\n",
    "# from TFIDF_train_data import DataProcessing\n",
    "# import TFIDF_test_data\n",
    "\n",
    "\n",
    "def find_index(word, phrase):\n",
    "    i = 0\n",
    "    for w in phrase:\n",
    "        if w == word:\n",
    "            return(i)\n",
    "            break\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"task05-TRAIN/train/C-41.txt\", \"r\")\n",
    "count = 0\n",
    "dat1 = np.array([])\n",
    " \n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    dat1 = np.append(dat1, line.strip())\n",
    "\n",
    "my_str = \"\"\n",
    "for line in dat1:\n",
    "    my_str = my_str + \" \" + str(line)\n",
    "# my_str = my_str.split(\". \")\n",
    "# dat1 = np.array(my_str)\n",
    "\n",
    "# candidate = np.array([])\n",
    "# for sen in dat1:\n",
    "#     candidate_phrase = ExtractNP(sen)\n",
    "#     candidate = np.append(candidate, candidate_phrase)\n",
    "\n",
    "candidate = ExtractCandidate(my_str)\n",
    "candidate_phrases = candidate.extract_candidate_chunks()\n",
    "# print(candidate.extract_candidate_chunks())\n",
    "candidate = candidate.CleaningCandidatePhrases(candidate_phrases) # To make the phrase of the basic form\n",
    "\n",
    "Vocabulary, idf_of_vocabulary = fit(candidate) \n",
    "\n",
    "final_output = transform(candidate, Vocabulary, idf_of_vocabulary) # TF-IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Similar phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation (root, final_output, candidate):\n",
    "\n",
    "    similar_candidate = []\n",
    "    sen_ind = []\n",
    "    sen_number = 0\n",
    "    for sen in candidate:\n",
    "        if sen.split().count(root)>0:\n",
    "            similar_candidate.append(sen)\n",
    "            sen_ind.append(sen_number)\n",
    "        sen_number = sen_number + 1\n",
    "    # the first element will be used to construct a tree, and the rest of it will be used to grow/prune the tree\n",
    "\n",
    "    # # Solution for (1) & (2): removing dulicate words & the non-alphabetic words from each sentence in similar candidate phrases\n",
    "    # similar_candidate_clean = []\n",
    "    # for sen in similar_candidate:\n",
    "    #     words = sen.split() # [( ), ( ), ..., ( )] \n",
    "    #     sen = \" \".join(sorted(set(words), key=words.index)) # removing dulicate words\n",
    "    #     sen_clean = []\n",
    "    #     for word in sen.split():\n",
    "    #         if (word.isalpha() or word.isspace()) == True:\n",
    "    #             sen_clean.append(word)\n",
    "    #     sen_clean = \" \".join(str(e) for e in sen_clean)\n",
    "    #     similar_candidate_clean.append(sen_clean)\n",
    "    # similar_candidate = similar_candidate_clean\n",
    "\n",
    "    # Solution for (1): removing dulicate words\n",
    "    similar_candidate_clean = []\n",
    "    for sen in similar_candidate:\n",
    "        words = sen.split() # [( ), ( ), ..., ( )] \n",
    "        sen = \" \".join(sorted(set(words), key=words.index)) # removing dulicate words\n",
    "        similar_candidate_clean.append(sen)\n",
    "    similar_candidate = similar_candidate_clean\n",
    "\n",
    "    # Find TF-IDF values for the similar candidate\n",
    "    index_matrix = np.array(final_output.nonzero())\n",
    "    similar_TFIDF = []\n",
    "    for row_ind in sen_ind:\n",
    "        ind = (index_matrix[0,] == row_ind) # find the corresponding phrase\n",
    "        col_ind = index_matrix[1, ind]\n",
    "        tf_idf_k = np.asarray(final_output[row_ind, col_ind].todense()).flatten()\n",
    "        similar_TFIDF.append(tf_idf_k)\n",
    "\n",
    "    # Solution for (2) & (3): Removing the problematic sentences for now\n",
    "    remove_list_ind = []\n",
    "    for l in range(len(similar_candidate)):\n",
    "        sim_sen = similar_candidate[l].split()\n",
    "        logic = (len(sim_sen) == len(similar_TFIDF[l]))\n",
    "        if logic == False:\n",
    "            # print('[phrase ' + str(l) + '] number of words: ' + str(len(similar_candidate[l].split())) + ', number of TFIDFs: ' + str(len(similar_TFIDF[l])))\n",
    "            remove_list_ind.append(l)\n",
    "    remove_list_ind.sort(reverse=True, key=int)\n",
    "    for i in remove_list_ind:\n",
    "        del similar_candidate[int(i)]\n",
    "        del similar_TFIDF[int(i)]\n",
    "\n",
    "    # print(similar_TFIDF)\n",
    "    # print(similar_candidate[0].split())\n",
    "    sigma1_index = similar_candidate[0].split().index(root)\n",
    "    root_tfidf = similar_TFIDF[0][sigma1_index]\n",
    "\n",
    "    return similar_candidate, similar_TFIDF, root_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resourc', 'system', 'applic', 'hyarm', 'video', 'util', 'dre', 'uav', 'adapt', 'receiv', 'requir', 'station', 'base', 'multimedia', 'control', 'resolut', 'figur', 'network', 'bandwidth', 'rate', 'end', 'frame', 'monitor', 'section', 'manag', 'jitter', 'result', 'latenc', 'real-tim', 'avail', 'perform', 'middlewar', 'experi', 'compress', 'tao', 'end-to-end', 'workload', 'power', 'pictur', 'paramet', 'servic', 'variabl', 'class', 'dynam', 'bound', 'comput', 'comparison', 'condit', 'architectur', 'link', 'ieee', 'qualiti', 'techniqu', 'cpu', 'case', 'studi', 'scheme', 'time', 'fluctuat', 'current', 'algorithm']\n"
     ]
    }
   ],
   "source": [
    "TF = NounTFCalculation()\n",
    "TF = TF.NounTF(candidate)\n",
    "# print(TF) # possible roots?\n",
    "\n",
    "sorted_TF = sorted(TF, key=lambda tf: tf[1], reverse=True)\n",
    "root_word_list = [x[0] for x in sorted_TF]\n",
    "print(root_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF = NounTFCalculation()\n",
    "\n",
    "# sorted_TF = sorted(TF, key=lambda tf: tf[1], reverse=True)\n",
    "# #print(sorted_TF)\n",
    "\n",
    "# root_list = [x[0] for x in sorted_TF]\n",
    "\n",
    "# # Remove the root words that contain non-alphabetic components \n",
    "# new_root_list = []\n",
    "# for root in root_list: \n",
    "#     if (root.isalpha() or root.isspace()) == True: \n",
    "#         new_root_list.append(root)\n",
    "# #print(len(new_root_list))\n",
    "\n",
    "# root_list = new_root_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMuValueFromTree(root, mu_dict):\n",
    "        if root:\n",
    "            GetMuValueFromTree(root.prev, mu_dict)\n",
    "            mu_dict[root.word] = root.mu\n",
    "            GetMuValueFromTree(root.next, mu_dict)\n",
    "\n",
    "def FindPhraseInCandidate(phrase, candidate_phrases):\n",
    "        if len(phrase) > 0:\n",
    "            for i in range (0, len(candidate_phrases)):\n",
    "                if phrase == candidate_phrases[i]:\n",
    "                    return i\n",
    "            return -1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration completed...\n",
      "2th iteration completed...\n",
      "3th iteration completed...\n",
      "4th iteration completed...\n",
      "5th iteration completed...\n",
      "6th iteration completed...\n",
      "7th iteration completed...\n",
      "8th iteration completed...\n",
      "9th iteration completed...\n",
      "10th iteration completed...\n",
      "11th iteration completed...\n",
      "12th iteration completed...\n",
      "13th iteration completed...\n",
      "14th iteration completed...\n",
      "15th iteration completed...\n",
      "16th iteration completed...\n",
      "17th iteration completed...\n",
      "18th iteration completed...\n",
      "19th iteration completed...\n",
      "20th iteration completed...\n",
      "21th iteration completed...\n",
      "22th iteration completed...\n",
      "23th iteration completed...\n",
      "24th iteration completed...\n",
      "25th iteration completed...\n",
      "26th iteration completed...\n",
      "27th iteration completed...\n",
      "28th iteration completed...\n",
      "29th iteration completed...\n",
      "30th iteration completed...\n",
      "31th iteration completed...\n",
      "32th iteration completed...\n",
      "33th iteration completed...\n",
      "34th iteration completed...\n",
      "35th iteration completed...\n",
      "36th iteration completed...\n",
      "37th iteration completed...\n",
      "38th iteration completed...\n",
      "39th iteration completed...\n",
      "40th iteration completed...\n",
      "41th iteration completed...\n",
      "42th iteration completed...\n",
      "43th iteration completed...\n",
      "44th iteration completed...\n",
      "45th iteration completed...\n",
      "46th iteration completed...\n",
      "47th iteration completed...\n",
      "48th iteration completed...\n",
      "49th iteration completed...\n",
      "50th iteration completed...\n",
      "51th iteration completed...\n",
      "52th iteration completed...\n",
      "53th iteration completed...\n",
      "54th iteration completed...\n",
      "55th iteration completed...\n",
      "56th iteration completed...\n",
      "57th iteration completed...\n",
      "58th iteration completed...\n",
      "59th iteration completed...\n",
      "60th iteration completed...\n",
      "61th iteration completed...\n"
     ]
    }
   ],
   "source": [
    "my_final_list = []\n",
    "weights = []\n",
    "my_final_dict = {}\n",
    "\n",
    "mu = -30\n",
    "\n",
    "for i in range(len(root_word_list)):\n",
    "    # print(\"i = \" + str(i))\n",
    "    root_word = root_word_list[i]\n",
    "    similar_candidate, similar_TFIDF, root_tfidf = preparation(root = root_word, final_output = final_output, candidate = candidate)\n",
    "\n",
    "    rootphrase = similar_candidate[0].split()\n",
    "    rootIndex = find_index(root_word, rootphrase)\n",
    "\n",
    "    my_tree = Tree()\n",
    "    my_tree.AddNode(word = root_word, phrase = rootphrase, wordIndex = rootIndex, rootIndex = rootIndex, tfidf = root_tfidf) # Add the root first\n",
    "\n",
    "    for j in range(len(similar_candidate)):\n",
    "        phrase = similar_candidate[j].split()\n",
    "        rootIndex = find_index(root_word, phrase)   # need to find rootIndex in each phrase\n",
    "        # print(\"j = \" + str(j))\n",
    "        for wordIndex in range(len(phrase)):\n",
    "            word = phrase[wordIndex]                    # extract the first word from the phrase with wordIndex\n",
    "            word_tfidf = similar_TFIDF[j][wordIndex]    # selecting the first sentence\n",
    "            \n",
    "            my_tree.AddNode(word, phrase, wordIndex, rootIndex, word_tfidf) \n",
    "            # my_tree.PrintTree(my_tree.root)\n",
    "            # print(str(wordIndex) + \"th iteration completed...\")\n",
    "\n",
    "        my_tree.UpdateMuValues(phrase)\n",
    "        \n",
    "    # my_tree.PrintTree(my_tree.root)\n",
    "    # my_NodeList = my_tree.CreateNodeList(my_tree.root)\n",
    "    # print(my_NodeList)\n",
    "\n",
    "    # Getting the final phrases list\n",
    "    final_node_list = []\n",
    "    final_node_list = my_tree.FindNodeListToExtractKeyPhrases(mu = mu, final_node_list = final_node_list, candidate_phrases= candidate) # This includes tree pruning too\n",
    "    final_phrases = []\n",
    "    for j in range(0, len(final_node_list)):\n",
    "        final_phrases.append(my_tree.GetPhrase(final_node_list[j]))\n",
    "    # my_final_list.append(final_phrases)\n",
    "\n",
    "\n",
    "    ## Getting the final mu list\n",
    "\n",
    "    # Get the mu value for each word\n",
    "    mu_dict = {} # a dictionary that saves \"word : mu\" pairs \n",
    "    GetMuValueFromTree(my_tree.root, mu_dict)\n",
    "\n",
    "    # Save the final keyphrases and their weights\n",
    "    for l in range(0, len(final_phrases)):\n",
    "        k = FindPhraseInCandidate(final_phrases[l], similar_candidate)\n",
    "        if  k == -1: \n",
    "            continue\n",
    "        else:\n",
    "            my_final_list.append(final_phrases[l])\n",
    "\n",
    "            # calculate the sum of tfidf of all the words in the final keyphrase\n",
    "            tfidf_value = np.sum(similar_TFIDF[k])\n",
    "            # calculate the sum of mu of all the words in the final keyphrase\n",
    "            mu_value = 0 \n",
    "            for j in final_phrases[l].split():\n",
    "                mu_value = mu_value + mu_dict[j]\n",
    "            weights.append(tfidf_value * mu_value)\n",
    "    \n",
    "    for m in range(0, len(my_final_list)): \n",
    "        my_final_dict[my_final_list[m]] = weights[m]\n",
    "    print(str(i+1) + \"th iteration completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'resourc': 181.0,\n",
       " 'system': 178.0,\n",
       " 'applic symp': 147.31221013038078,\n",
       " 'applic': 135.0,\n",
       " 'base station': 132.93574843690024,\n",
       " 'hyarm structur': 131.40603785467903,\n",
       " 'video latenc figur': 121.95919083874473,\n",
       " 'dre multimedia system': 114.88136377578579,\n",
       " 'hyarm': 107.0,\n",
       " 'video': 97.0,\n",
       " 'util': 96.0,\n",
       " 'repres dre multimedia system': 93.11674654616422,\n",
       " 'multimedia system': 78.08135472617971,\n",
       " 'uav node': 76.60993277831403,\n",
       " 'frame rate': 73.53746636560592,\n",
       " 'end receiv': 69.13582647957186,\n",
       " 'network bandwidth': 65.04662414913935,\n",
       " 'applic adapt': 64.65588942298481,\n",
       " 'network bandwidth util': 63.712636264523965,\n",
       " 'dre': 57.0,\n",
       " 'uav': 54.0,\n",
       " 'adapt': 54.0,\n",
       " 'receiv video': 53.58401800308933,\n",
       " 'resourc avail and/or demand': 52.19875699245194,\n",
       " 'control variabl': 50.70028498056856,\n",
       " 'receiv': 50.0,\n",
       " 'requir': 50.0,\n",
       " 'station': 48.0,\n",
       " 'video frame rate': 46.42799783720997,\n",
       " 'higher frame rate': 46.287393229987735,\n",
       " 'pictur resolut': 45.09721823393047,\n",
       " 'multimedia': 45.0,\n",
       " 'control': 41.0,\n",
       " 'resolut': 40.0,\n",
       " 'figur': 39.0,\n",
       " 'resourc avail': 37.00079655289395,\n",
       " 'network': 35.0,\n",
       " 'network bandwidth monitor': 32.90218497955517,\n",
       " 'adapt middlewar': 32.33248038618815,\n",
       " 'compress scheme': 31.100488029954334,\n",
       " 'variou compress scheme': 31.0117996954517,\n",
       " 'bandwidth': 30.0,\n",
       " 'bandwidth util': 29.465779303625073,\n",
       " 'case studi': 28.2842712474619,\n",
       " 'comput power': 26.868473570655322,\n",
       " 'continu control': 26.61644933979757,\n",
       " 'monitor': 26.0,\n",
       " 'section': 26.0,\n",
       " 'system condit': 25.898067747759917,\n",
       " 'end': 25.0,\n",
       " 'resourc monitor': 24.856231707175805,\n",
       " 'class resolut frame rate latenc': 24.552877930499804,\n",
       " 'continu control variabl': 24.095729070161337,\n",
       " 'jitter': 24.0,\n",
       " 'desir system condit': 21.684610869121393,\n",
       " 'ieee real-tim': 21.199760467694066,\n",
       " 'hyarm figur': 21.07618012536429,\n",
       " 'result': 21.0,\n",
       " 'latenc': 21.0,\n",
       " 'system perform': 20.571117709220978,\n",
       " '10th ieee real-tim': 18.88469632971273,\n",
       " 'avail': 18.0,\n",
       " 'perform': 18.0,\n",
       " 'experi': 17.0,\n",
       " 'compress': 17.0,\n",
       " 'monitor system resourc': 16.90460650400278,\n",
       " 'video paramet': 16.697631652698153,\n",
       " 'workload': 16.0,\n",
       " 'power': 16.0,\n",
       " 'paramet': 16.0,\n",
       " 'tao': 15.0,\n",
       " 'servic': 15.0,\n",
       " 'variabl': 15.0,\n",
       " 'class': 15.0,\n",
       " 'bound': 14.0,\n",
       " 'comparison': 14.0,\n",
       " 'certain class': 13.979707020469297,\n",
       " 'poor qualiti video': 13.382489643920591,\n",
       " 'condit': 13.0,\n",
       " 'architectur': 13.0,\n",
       " 'ieee': 13.0,\n",
       " 'g. tao': 12.640825274985689,\n",
       " 'empir result': 12.578263321313818,\n",
       " 'video compress': 12.533827437821985,\n",
       " 'compress video': 12.533827437821985,\n",
       " 'qualiti': 12.0,\n",
       " 'techniqu': 12.0,\n",
       " 'cpu': 12.0,\n",
       " 'studi': 12.0,\n",
       " 'scheme': 12.0,\n",
       " 'time': 12.0,\n",
       " 'workload condit': 11.312122747525576,\n",
       " 'autonom comput': 11.231290320449904,\n",
       " 'fluctuat': 11.0,\n",
       " 'algorithm': 11.0,\n",
       " 'experi configur': 9.84847198747367,\n",
       " 'network link': 9.835166375542196,\n",
       " 'link': 9.0,\n",
       " 'specifi util bound': 8.458399870103602,\n",
       " 'sudden fluctuat': 8.415545985125316,\n",
       " 'dynam factor': 7.01955645028119,\n",
       " 'compar latenc': 6.987924067396566,\n",
       " 'differenti servic': 5.611882099086362,\n",
       " 'wireless network link': 5.159919842723473,\n",
       " 'compress techniqu': 4.240975640448318,\n",
       " 'discret dynam': 4.237092831910735,\n",
       " 'discret variabl': 4.235826758101172,\n",
       " 'cpu monitor': 4.232667638203741,\n",
       " 'residu cpu': 4.225613351163711,\n",
       " 'predict system perform': 1.6628121616819214,\n",
       " 'real-tim version': 1.3927108532880679,\n",
       " 'compress video signal': -1.6800183555240127,\n",
       " 'current level': -5.610363990083544,\n",
       " 'multimedia system architectur wireless link': -7.372951205084089,\n",
       " 'rate monoton algorithm': -8.561233558824178,\n",
       " 'end-to-end qualiti': -11.310720065245341,\n",
       " 'perform result and analysi': -23.71015186048573,\n",
       " 'rate monoton schedul algorithm': -29.732689252686534}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_final_dict_sorted = dict(sorted(my_final_dict.items(), key=lambda item: item[1], reverse = True))\n",
    "my_final_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = 0\n",
    "# cnt = 0\n",
    "# for c in range(len(candidate)):\n",
    "#     sent = candidate[c]\n",
    "#     if sent == 'resourc':\n",
    "#         cnt = cnt + 1\n",
    "#         # print(ind)\n",
    "#     ind = ind + 1\n",
    "# print(cnt)\n",
    "\n",
    "# candidate[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'applic symp': 147.31221013038078,\n",
       " 'base station': 132.93574843690024,\n",
       " 'hyarm structur': 131.40603785467903,\n",
       " 'video latenc figur': 121.95919083874473,\n",
       " 'dre multimedia system': 114.88136377578579,\n",
       " 'repres dre multimedia system': 93.11674654616422,\n",
       " 'multimedia system': 78.08135472617971,\n",
       " 'uav node': 76.60993277831403,\n",
       " 'frame rate': 73.53746636560592,\n",
       " 'end receiv': 69.13582647957186,\n",
       " 'network bandwidth': 65.04662414913935,\n",
       " 'applic adapt': 64.65588942298481,\n",
       " 'network bandwidth util': 63.712636264523965,\n",
       " 'receiv video': 53.58401800308933,\n",
       " 'resourc avail and/or demand': 52.19875699245194,\n",
       " 'control variabl': 50.70028498056856,\n",
       " 'video frame rate': 46.42799783720997,\n",
       " 'higher frame rate': 46.287393229987735,\n",
       " 'pictur resolut': 45.09721823393047,\n",
       " 'resourc avail': 37.00079655289395,\n",
       " 'network bandwidth monitor': 32.90218497955517,\n",
       " 'adapt middlewar': 32.33248038618815,\n",
       " 'compress scheme': 31.100488029954334,\n",
       " 'variou compress scheme': 31.0117996954517,\n",
       " 'bandwidth util': 29.465779303625073,\n",
       " 'case studi': 28.2842712474619,\n",
       " 'comput power': 26.868473570655322,\n",
       " 'continu control': 26.61644933979757,\n",
       " 'system condit': 25.898067747759917,\n",
       " 'resourc monitor': 24.856231707175805,\n",
       " 'class resolut frame rate latenc': 24.552877930499804,\n",
       " 'continu control variabl': 24.095729070161337,\n",
       " 'desir system condit': 21.684610869121393,\n",
       " 'ieee real-tim': 21.199760467694066,\n",
       " 'hyarm figur': 21.07618012536429,\n",
       " 'system perform': 20.571117709220978,\n",
       " '10th ieee real-tim': 18.88469632971273,\n",
       " 'monitor system resourc': 16.90460650400278,\n",
       " 'video paramet': 16.697631652698153,\n",
       " 'certain class': 13.979707020469297,\n",
       " 'poor qualiti video': 13.382489643920591,\n",
       " 'g. tao': 12.640825274985689,\n",
       " 'empir result': 12.578263321313818,\n",
       " 'video compress': 12.533827437821985,\n",
       " 'compress video': 12.533827437821985,\n",
       " 'workload condit': 11.312122747525576,\n",
       " 'autonom comput': 11.231290320449904,\n",
       " 'experi configur': 9.84847198747367,\n",
       " 'network link': 9.835166375542196,\n",
       " 'specifi util bound': 8.458399870103602,\n",
       " 'sudden fluctuat': 8.415545985125316,\n",
       " 'dynam factor': 7.01955645028119,\n",
       " 'compar latenc': 6.987924067396566,\n",
       " 'differenti servic': 5.611882099086362,\n",
       " 'wireless network link': 5.159919842723473,\n",
       " 'compress techniqu': 4.240975640448318,\n",
       " 'discret dynam': 4.237092831910735,\n",
       " 'discret variabl': 4.235826758101172,\n",
       " 'cpu monitor': 4.232667638203741,\n",
       " 'residu cpu': 4.225613351163711,\n",
       " 'predict system perform': 1.6628121616819214,\n",
       " 'real-tim version': 1.3927108532880679,\n",
       " 'compress video signal': -1.6800183555240127,\n",
       " 'current level': -5.610363990083544,\n",
       " 'multimedia system architectur wireless link': -7.372951205084089,\n",
       " 'rate monoton algorithm': -8.561233558824178,\n",
       " 'end-to-end qualiti': -11.310720065245341,\n",
       " 'perform result and analysi': -23.71015186048573,\n",
       " 'rate monoton schedul algorithm': -29.732689252686534}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = list(my_final_dict_sorted.keys())\n",
    "value = list(my_final_dict_sorted.values())\n",
    "\n",
    "final_real_phrases = {}\n",
    "for i in range(len(key)):\n",
    "    if len(key[i].split()) > 1:\n",
    "        final_real_phrases[key[i]] = value[i]\n",
    "final_real_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Using ExtractCandidate()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['applic symp',\n",
       " 'base station',\n",
       " 'hyarm structur',\n",
       " 'video latenc figur',\n",
       " 'dre multimedia system',\n",
       " 'repres dre multimedia system',\n",
       " 'multimedia system',\n",
       " 'uav node',\n",
       " 'frame rate',\n",
       " 'end receiv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "N_phrases = list(final_real_phrases.keys())[0:N]\n",
    "print(\"(1) Using ExtractCandidate()\")\n",
    "N_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Standard (Stem version)\n",
    "\n",
    "[author]\n",
    "distribut real-time emb system,hybrid system,qualiti of servic+servic qualiti\n",
    "\n",
    "[reader]\n",
    "adapt resourc manag,distribut real-time embed system,end-to-end qualiti of servic+servic end-to-end qualiti,hybrid adapt resourcemanag middlewar,hybrid control techniqu,real-time video distribut system,real-time corba specif,video encod/decod,resourc reserv mechan,dynam environ,stream servic\n",
    "\n",
    "\n",
    "[Combined]\n",
    "adapt resourc manag,distribut real-time embed system,end-to-end qualiti of servic+servic end-to-end qualiti,hybrid adapt resourcemanag middlewar,hybrid control techniqu,real-time video distribut system,real-time corba specif,video encod/decod,resourc reserv mechan,dynam environ,stream servic,distribut real-time emb system,hybrid system,qualiti of servic+servic qualiti"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "025ad61b930c2538e978b72ab3450d0e2eb24a8c556b2730cae258c3bfbd449a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
